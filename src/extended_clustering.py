import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os
import time
import sys
import warnings
warnings.filterwarnings('ignore')
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.metrics import silhouette_score
# å¯¼å…¥è°±èšç±»åŠŸèƒ½
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from spectral_clustering import spectral_clustering

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei']  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºä¸­æ–‡æ ‡ç­¾
plt.rcParams['axes.unicode_minus'] = False  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºè´Ÿå·

def load_data(sample_size=5000):
    """
    åŠ è½½NHANESå¤„ç†åçš„æ•°æ®é›†
    """
    # è·å–å½“å‰æ–‡ä»¶çš„ç›®å½•
    current_dir = os.path.dirname(os.path.abspath(__file__))
    # è·å–æ•°æ®æ–‡ä»¶è·¯å¾„
    data_dir = os.path.join(os.path.dirname(current_dir), "data")
    data_path = os.path.join(data_dir, "nhanes_processed.csv")
    
    print("æ­£åœ¨åŠ è½½æ•°æ®...")
    df = pd.read_csv(data_path, nrows=sample_size)
    print(f"æ•°æ®åŠ è½½å®Œæˆï¼Œå½¢çŠ¶: {df.shape}")
    return df

def preprocess_data(df):
    """
    é¢„å¤„ç†æ•°æ®ï¼šé€‰æ‹©ç‰¹å¾ã€å¤„ç†ç¼ºå¤±å€¼ã€æ ‡å‡†åŒ–
    """
    print("å¼€å§‹æ•°æ®é¢„å¤„ç†...")
    
    # é€‰æ‹©æœ‰ç”¨çš„ç‰¹å¾åˆ— (å¹´é¾„ã€æ€§åˆ«ã€BMIã€è¡€å‹ã€è¡€ç³–ã€èƒ†å›ºé†‡ç­‰)
    # æ ¸å¿ƒç‰¹å¾å¿…é¡»æœ‰
    must_have_features = [
        'RIDAGEYR',     # å¹´é¾„
        'RIAGENDR',     # æ€§åˆ«
    ]
    
    # å…¶ä»–å¥åº·æŒ‡æ ‡
    health_indicators = [
        'BMXBMI',       # BMI
        'BPXSY1',       # æ”¶ç¼©å‹
        'BPXDI1',       # èˆ’å¼ å‹
        'LBXTC',        # æ€»èƒ†å›ºé†‡
        'LBXTR',        # ç”˜æ²¹ä¸‰é…¯
        'SMQ020',       # å¸çƒŸçŠ¶å†µ
        'ALQ101'        # é¥®é…’çŠ¶å†µ
    ]
    
    # å¯é€‰çš„è¡€ç³–å’Œèƒ†å›ºé†‡æŒ‡æ ‡
    optional_glucose = ['LBXGLU', 'LBXGH', 'LBXGLT']  # ä¸åŒå½¢å¼çš„è¡€ç³–æŒ‡æ ‡
    optional_cholesterol = ['LBXHDL', 'LBDLDL']  # ä¸åŒå½¢å¼çš„èƒ†å›ºé†‡æŒ‡æ ‡
    
    # ç¡®ä¿å¿…é€‰ç‰¹å¾å­˜åœ¨
    for feature in must_have_features:
        if feature not in df.columns:
            raise ValueError(f"å¿…éœ€çš„ç‰¹å¾ {feature} ä¸å­˜åœ¨äºæ•°æ®é›†ä¸­")
    
    # æ·»åŠ å¯ç”¨çš„å¥åº·æŒ‡æ ‡
    available_features = must_have_features.copy()
    available_features.extend([col for col in health_indicators if col in df.columns])
    
    # å°è¯•æ·»åŠ è¡€ç³–å’Œèƒ†å›ºé†‡æŒ‡æ ‡
    for glucose in optional_glucose:
        if glucose in df.columns:
            available_features.append(glucose)
            print(f"é€‰æ‹©äº†è¡€ç³–æŒ‡æ ‡: {glucose}")
            break
    
    for cholesterol in optional_cholesterol:
        if cholesterol in df.columns:
            available_features.append(cholesterol)
            print(f"é€‰æ‹©äº†èƒ†å›ºé†‡æŒ‡æ ‡: {cholesterol}")
            break
    
    print(f"é€‰æ‹©çš„ç‰¹å¾: {available_features}")
    
    # é€‰æ‹©æ•°æ®
    data = df[available_features].copy()
    
    # æ£€æŸ¥ç¼ºå¤±å€¼
    print("ç¼ºå¤±å€¼ç»Ÿè®¡:")
    print(data.isnull().sum())
    
    # å»é™¤å«æœ‰ç¼ºå¤±å€¼çš„è¡Œ
    data = data.dropna()
    print(f"å¤„ç†ç¼ºå¤±å€¼åçš„æ•°æ®å½¢çŠ¶: {data.shape}")
    
    # æ ‡å‡†åŒ–æ•°æ®
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(data)
    
    return data, scaled_data, available_features

def reduce_dimensions(scaled_data):
    """
    ä½¿ç”¨PCAé™ç»´ï¼Œä¾¿äºå¯è§†åŒ–
    """
    print("æ­£åœ¨è¿›è¡ŒPCAé™ç»´...")
    pca = PCA(n_components=2)
    reduced_data = pca.fit_transform(scaled_data)
    print(f"PCAè§£é‡Šæ–¹å·®æ¯”ä¾‹: {pca.explained_variance_ratio_}")
    
    return reduced_data

def kmeans_clustering(scaled_data, n_clusters=5):
    """
    Kå‡å€¼èšç±»
    """
    print(f"\nğŸ” æ‰§è¡ŒKå‡å€¼èšç±»åˆ†æ (k={n_clusters}) " + "="*40)
    print(f"  Â· ç®—æ³•ç‰¹ç‚¹: ç®€å•é«˜æ•ˆï¼Œé€‚åˆå‘ç°çƒçŠ¶ç°‡")
    print(f"  Â· ä¼˜åŠ¿: è®¡ç®—é€Ÿåº¦å¿«ï¼Œæ˜“äºç†è§£å’Œå®ç°")
    print(f"  Â· åŠ£åŠ¿: å¯¹åˆå§‹è´¨å¿ƒæ•æ„Ÿï¼Œåªèƒ½å‘ç°å‡¸å½¢çŠ¶çš„ç°‡")
    
    # å¼€å§‹è®¡æ—¶
    start_time = time.time()
    
    # æ‰§è¡Œèšç±»
    print(f"\n  å¼€å§‹è®­ç»ƒKå‡å€¼æ¨¡å‹...")
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init="auto")
    clusters = kmeans.fit_predict(scaled_data)
    
    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    end_time = time.time()
    silhouette_avg = silhouette_score(scaled_data, clusters)
    
    # ç»Ÿè®¡æ¯ä¸ªç°‡çš„æ ·æœ¬æ•°
    unique, counts = np.unique(clusters, return_counts=True)
    cluster_counts = dict(zip(unique, counts))
    
    # è¾“å‡ºç»“æœ
    print(f"\n  ğŸ“Š èšç±»ç»“æœ:")
    print(f"    - æ€»æ ·æœ¬æ•°: {len(clusters)}")
    print(f"    - èšç±»æ•°é‡: {n_clusters}")
    for i in range(n_clusters):
        print(f"    - ç¾¤ä½“ {i+1}: {cluster_counts.get(i, 0)}ä¸ªæ ·æœ¬ ({cluster_counts.get(i, 0)/len(clusters)*100:.1f}%)")
    print(f"\n  ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:")
    print(f"    - è½®å»“ç³»æ•°: {silhouette_avg:.4f}  (èŒƒå›´: -1åˆ°1, è¶Šå¤§è¶Šå¥½)")
    print(f"    - è¿è¡Œæ—¶é—´: {end_time - start_time:.2f}ç§’")
    
    return clusters

def hierarchical_clustering(scaled_data, n_clusters=5):
    """
    å±‚æ¬¡èšç±»
    """
    print(f"\nğŸ” æ‰§è¡Œå±‚æ¬¡èšç±»åˆ†æ (k={n_clusters}) " + "="*40)
    print(f"  Â· ç®—æ³•ç‰¹ç‚¹: è‡ªåº•å‘ä¸Šé€æ­¥åˆå¹¶æ ·æœ¬ï¼Œå½¢æˆå±‚æ¬¡ç»“æ„")
    print(f"  Â· ä¼˜åŠ¿: ä¸éœ€è¦é¢„å…ˆæŒ‡å®šèšç±»æ•°é‡ï¼Œèƒ½å¤Ÿå‘ç°åµŒå¥—å…³ç³»")
    print(f"  Â· åŠ£åŠ¿: è®¡ç®—å¤æ‚åº¦é«˜ï¼Œéš¾ä»¥å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†")
    
    # å¼€å§‹è®¡æ—¶
    start_time = time.time()
    
    # æ‰§è¡Œèšç±»
    print(f"\n  å¼€å§‹è®­ç»ƒå±‚æ¬¡èšç±»æ¨¡å‹...")
    hierarchical = AgglomerativeClustering(n_clusters=n_clusters)
    clusters = hierarchical.fit_predict(scaled_data)
    
    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    end_time = time.time()
    silhouette_avg = silhouette_score(scaled_data, clusters)
    
    # ç»Ÿè®¡æ¯ä¸ªç°‡çš„æ ·æœ¬æ•°
    unique, counts = np.unique(clusters, return_counts=True)
    cluster_counts = dict(zip(unique, counts))
    
    # è¾“å‡ºç»“æœ
    print(f"\n  ğŸ“Š èšç±»ç»“æœ:")
    print(f"    - æ€»æ ·æœ¬æ•°: {len(clusters)}")
    print(f"    - èšç±»æ•°é‡: {n_clusters}")
    for i in range(n_clusters):
        print(f"    - ç¾¤ä½“ {i+1}: {cluster_counts.get(i, 0)}ä¸ªæ ·æœ¬ ({cluster_counts.get(i, 0)/len(clusters)*100:.1f}%)")
    print(f"\n  ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:")
    print(f"    - è½®å»“ç³»æ•°: {silhouette_avg:.4f}  (èŒƒå›´: -1åˆ°1, è¶Šå¤§è¶Šå¥½)")
    print(f"    - è¿è¡Œæ—¶é—´: {end_time - start_time:.2f}ç§’")
    
    return clusters

def dbscan_clustering(scaled_data, eps=0.5, min_samples=5):
    """
    DBSCANèšç±»
    """
    print(f"\nğŸ” æ‰§è¡ŒDBSCANå¯†åº¦èšç±»åˆ†æ " + "="*40)
    print(f"  Â· ç®—æ³•ç‰¹ç‚¹: åŸºäºå¯†åº¦çš„ç©ºé—´èšç±»ï¼Œèƒ½è¯†åˆ«ä»»æ„å½¢çŠ¶çš„ç°‡")
    print(f"  Â· ä¼˜åŠ¿: èƒ½å‘ç°ä»»æ„å½¢çŠ¶çš„ç°‡ï¼Œè‡ªåŠ¨è¯†åˆ«å™ªå£°ç‚¹ï¼Œä¸éœ€è¦é¢„å…ˆæŒ‡å®šç°‡æ•°é‡")
    print(f"  Â· åŠ£åŠ¿: å¯¹å‚æ•°æ•æ„Ÿï¼Œå¤„ç†ä¸åŒå¯†åº¦çš„ç°‡æ•ˆæœè¾ƒå·®")
    print(f"  Â· å‚æ•°è®¾ç½®: eps={eps} (é‚»åŸŸåŠå¾„), min_samples={min_samples} (æœ€å°æ ·æœ¬æ•°)")
    
    # å¼€å§‹è®¡æ—¶
    start_time = time.time()
    
    # æ‰§è¡Œèšç±»
    print(f"\n  å¼€å§‹è®­ç»ƒDBSCANæ¨¡å‹...")
    dbscan = DBSCAN(eps=eps, min_samples=min_samples)
    clusters = dbscan.fit_predict(scaled_data)
    
    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    end_time = time.time()
    
    # ç»Ÿè®¡èšç±»ç»“æœ
    n_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)
    noise_count = np.sum(clusters == -1)
    noise_ratio = noise_count / len(clusters)
    
    # è®¡ç®—è½®å»“ç³»æ•°(å¦‚æœæœ‰æ•ˆ)
    has_valid_silhouette = len(np.unique(clusters)) > 1 and (len(clusters) - noise_count) > 1
    if has_valid_silhouette:
        # è®¡ç®—æ— å™ªå£°ç‚¹çš„è½®å»“ç³»æ•°
        valid_indices = clusters != -1
        if np.sum(valid_indices) > 1:
            try:
                silhouette_avg = silhouette_score(scaled_data[valid_indices], 
                                                 clusters[valid_indices])
            except:
                silhouette_avg = "æ— æ³•è®¡ç®—"
        else:
            silhouette_avg = "æ— æ³•è®¡ç®—"
    else:
        silhouette_avg = "æ— æ³•è®¡ç®—"
        
    # ç»Ÿè®¡æ¯ä¸ªç°‡çš„æ ·æœ¬æ•°
    unique, counts = np.unique(clusters, return_counts=True)
    cluster_counts = dict(zip(unique, counts))
    
    # è¾“å‡ºç»“æœ
    print(f"\n  ğŸ“Š èšç±»ç»“æœ:")
    print(f"    - æ€»æ ·æœ¬æ•°: {len(clusters)}")
    print(f"    - è‡ªåŠ¨è¯†åˆ«çš„ç¾¤ä½“æ•°é‡: {n_clusters}")
    print(f"    - å™ªå£°ç‚¹: {noise_count}ä¸ªæ ·æœ¬ ({noise_ratio*100:.1f}%)")
    
    # æ˜¾ç¤ºå„ä¸ªç°‡çš„æ ·æœ¬æ•°é‡å’Œç™¾åˆ†æ¯”
    for i in [k for k in cluster_counts.keys() if k != -1]:
        print(f"    - ç¾¤ä½“ {i+1}: {cluster_counts.get(i, 0)}ä¸ªæ ·æœ¬ ({cluster_counts.get(i, 0)/len(clusters)*100:.1f}%)")
    
    print(f"\n  ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:")
    print(f"    - è½®å»“ç³»æ•°: {silhouette_avg if isinstance(silhouette_avg, str) else silhouette_avg:.4f}")
    print(f"    - å™ªå£°ç‚¹æ¯”ä¾‹: {noise_ratio:.4f} (0-1ä¹‹é—´ï¼Œè¶Šå°è¶Šå¥½)")
    print(f"    - è¿è¡Œæ—¶é—´: {end_time - start_time:.2f}ç§’")
    
    # åˆ†æç»“æœè´¨é‡
    if noise_ratio > 0.5:
        print(f"\n  âš ï¸ è­¦å‘Š: DBSCANç”Ÿæˆè¿‡å¤šå™ªå£°ç‚¹ ({noise_ratio*100:.1f}%)ï¼Œèšç±»æ•ˆæœä¸ä½³")
        print(f"     å»ºè®®å°è¯•å¢åŠ epså€¼æˆ–å‡å°‘min_sampleså€¼")
    elif n_clusters < 2:
        print(f"\n  âš ï¸ è­¦å‘Š: DBSCANæœªèƒ½è¯†åˆ«è¶³å¤Ÿçš„ç¾¤ä½“ï¼Œå»ºè®®è°ƒæ•´å‚æ•°")
    elif n_clusters > 10:
        print(f"\n  âš ï¸ æç¤º: DBSCANè¯†åˆ«äº†è¾ƒå¤š({n_clusters})ä¸ªç¾¤ä½“ï¼Œå¯èƒ½éœ€è¦å¢åŠ min_samples")
    
    return clusters

def visualize_clusters(data, reduced_data, clusters, algorithm_name, features):
    """
    å¯è§†åŒ–èšç±»ç»“æœ
    """
    # åˆ›å»ºç»“æœç›®å½•
    current_dir = os.path.dirname(os.path.abspath(__file__))
    results_dir = os.path.join(os.path.dirname(current_dir), "results")
    if not os.path.exists(results_dir):
        os.makedirs(results_dir)
    
    # å°†èšç±»æ ‡ç­¾æ·»åŠ åˆ°åŸå§‹æ•°æ®ä¸­
    data_with_clusters = data.copy()
    data_with_clusters['Cluster'] = clusters
    
    # ä¿å­˜èšç±»ç»“æœ
    result_path = os.path.join(results_dir, f"{algorithm_name}_results.csv")
    data_with_clusters.to_csv(result_path, index=False)
    
    # 1. PCAå¯è§†åŒ–
    plt.figure(figsize=(12, 10))
    
    # åˆ›å»ºä¸€ä¸ªcolormap
    unique_clusters = np.unique(clusters)
    colors = plt.cm.get_cmap('tab10')(np.linspace(0, 1, len(unique_clusters)))
    
    # ç»˜åˆ¶æ•£ç‚¹å›¾
    for i, cluster in enumerate(unique_clusters):
        if cluster == -1:  # DBSCANçš„å™ªå£°ç‚¹
            plt.scatter(
                reduced_data[clusters == cluster, 0],
                reduced_data[clusters == cluster, 1],
                s=30, c='black', marker='x', label=f'å™ªå£°'
            )
        else:
            plt.scatter(
                reduced_data[clusters == cluster, 0],
                reduced_data[clusters == cluster, 1],
                s=30, c=[colors[i]], label=f'ç¾¤ä½“ {cluster+1}'
            )
    
    plt.title(f'{algorithm_name} èšç±»ç»“æœå¯è§†åŒ– (PCAé™ç»´)', fontsize=15)
    plt.xlabel('ä¸»æˆåˆ†1', fontsize=12)
    plt.ylabel('ä¸»æˆåˆ†2', fontsize=12)
    plt.legend(fontsize=10)
    plt.grid(True, linestyle='--', alpha=0.7)
    
    # ä¿å­˜å›¾ç‰‡
    plt.tight_layout()
    plt.savefig(os.path.join(results_dir, f"{algorithm_name}_pca_visualization.png"), dpi=300)
    plt.close()
    
    # åˆ†æç¾¤ä½“ç‰¹å¾
    analyze_clusters(data_with_clusters, algorithm_name, features)
    
def analyze_clusters(data_with_clusters, algorithm_name, features):
    """
    åˆ†æå„ç¾¤ä½“ç‰¹å¾å¹¶ç”Ÿæˆå¥åº·ç®¡ç†å»ºè®®
    """
    # åˆ›å»ºç»“æœç›®å½•
    current_dir = os.path.dirname(os.path.abspath(__file__))
    results_dir = os.path.join(os.path.dirname(current_dir), "results")
    
    # è®¡ç®—æ¯ä¸ªç¾¤ä½“çš„ç‰¹å¾ç»Ÿè®¡
    stats_path = os.path.join(results_dir, f"{algorithm_name}_cluster_statistics.csv")
    cluster_stats = data_with_clusters.groupby('Cluster').agg(['mean', 'std']).round(2)
    cluster_stats.to_csv(stats_path)
    
    # ä¸ºæ¯ä¸ªç¾¤ä½“ç”Ÿæˆå¥åº·ç®¡ç†å»ºè®®
    with open(os.path.join(results_dir, f"{algorithm_name}_health_recommendations.txt"), "w", encoding="utf-8") as f:
        f.write(f"=== {algorithm_name} èšç±»å¥åº·ç¾¤ä½“åˆ†æ ===\n\n")
        
        for cluster in sorted(data_with_clusters['Cluster'].unique()):
            cluster_data = data_with_clusters[data_with_clusters['Cluster'] == cluster]
            
            # è·³è¿‡å¯èƒ½çš„å™ªå£°ç‚¹
            if cluster == -1:
                f.write(f"ç¾¤ä½“: å™ªå£°ç‚¹ (æ ·æœ¬æ•°: {len(cluster_data)})\n")
                f.write("æè¿°: è¿™äº›æ ·æœ¬è¢«è¯†åˆ«ä¸ºå¼‚å¸¸å€¼ï¼Œä¸å±äºä»»ä½•ä¸»è¦å¥åº·ç¾¤ä½“\n")
                f.write("å»ºè®®: éœ€è¦ä¸ªä½“åŒ–è¯„ä¼°\n\n")
                continue
            
            try:
                mean_age = cluster_data['RIDAGEYR'].mean()
                mean_gender = cluster_data['RIAGENDR'].mean()  # 1=ç”·æ€§, 2=å¥³æ€§
                gender_str = "ç”·æ€§ä¸ºä¸»" if mean_gender < 1.5 else "å¥³æ€§ä¸ºä¸»"
                
                # BMIå’Œè¡€å‹
                has_bmi = 'BMXBMI' in cluster_data.columns
                has_bp = 'BPXSY1' in cluster_data.columns and 'BPXDI1' in cluster_data.columns
                
                mean_bmi = cluster_data['BMXBMI'].mean() if has_bmi else "æœªçŸ¥"
                mean_systolic = cluster_data['BPXSY1'].mean() if has_bp else "æœªçŸ¥"
                mean_diastolic = cluster_data['BPXDI1'].mean() if has_bp else "æœªçŸ¥"
                
                # æ ¹æ®ç‰¹å¾å€¼è¯†åˆ«å¥åº·ç¾¤ä½“ç±»å‹
                group_type = ""
                health_issues = []
                recommendations = []
                
                # è¯†åˆ«å¹´é¾„ç»„
                if mean_age < 30:
                    group_type = "é’å¹´å¥åº·ç¾¤ä½“"
                    recommendations.extend([
                        "1. å»ºç«‹è‰¯å¥½çš„ç”Ÿæ´»ä¹ æƒ¯ï¼ŒåŒ…æ‹¬è§„å¾‹ä½œæ¯ã€å‡è¡¡é¥®é£Ÿ",
                        "2. å®šæœŸä½“æ£€ï¼Œå»ºç«‹å¥åº·æ¡£æ¡ˆ",
                        "3. å¢åŠ ä½“è‚²é”»ç‚¼ï¼Œå¢å¼ºä½“è´¨",
                        "4. æ³¨æ„å¿ƒç†å¥åº·ï¼Œä¿æŒç§¯æå¿ƒæ€"
                    ])
                elif mean_age > 65:
                    group_type = "è€å¹´å¥åº·ç¾¤ä½“"
                    recommendations.extend([
                        "1. å®šæœŸä½“æ£€ï¼Œç‰¹åˆ«å…³æ³¨å¿ƒè„‘è¡€ç®¡å¥åº·",
                        "2. é€‚åº¦é”»ç‚¼ï¼Œå¦‚æ•£æ­¥ã€å¤ªæç­‰ä½å¼ºåº¦è¿åŠ¨",
                        "3. ä¿æŒç¤¾äº¤æ´»åŠ¨ï¼Œé¢„é˜²è®¤çŸ¥åŠŸèƒ½ä¸‹é™",
                        "4. åˆç†ç”¨è¯ï¼Œé¿å…è¯ç‰©ç›¸äº’ä½œç”¨",
                        "5. é˜²è·Œå€’æªæ–½ï¼Œä¿éšœå±…å®¶å®‰å…¨"
                    ])
                else:
                    group_type = "ä¸­å¹´å¥åº·ç¾¤ä½“"
                    recommendations.extend([
                        "1. å®šæœŸä½“æ£€ï¼Œå…³æ³¨æ…¢æ€§ç—…é£é™©",
                        "2. ä¿æŒè§„å¾‹ä½œæ¯ï¼Œé¿å…è¿‡åº¦åŠ³ç´¯",
                        "3. å‡è¡¡é¥®é£Ÿï¼Œæ§åˆ¶çƒ­é‡æ‘„å…¥",
                        "4. åšæŒé”»ç‚¼ï¼Œæ¯å‘¨è‡³å°‘150åˆ†é’Ÿä¸­ç­‰å¼ºåº¦æ´»åŠ¨"
                    ])
                
                # è¯„ä¼°BMI
                if has_bmi:
                    bmi_status = ""
                    if isinstance(mean_bmi, (int, float)) and mean_bmi < 18.5:
                        bmi_status = "ä½“é‡è¿‡è½»"
                        health_issues.append("ä½“é‡è¿‡è½»")
                        recommendations.append("5. å¢åŠ è¥å…»æ‘„å…¥ï¼Œé€‚å½“å¢åŠ ä½“é‡")
                    elif isinstance(mean_bmi, (int, float)) and mean_bmi < 24:
                        bmi_status = "æ­£å¸¸ä½“é‡"
                    elif isinstance(mean_bmi, (int, float)) and mean_bmi < 28:
                        bmi_status = "è¶…é‡"
                        health_issues.append("è¶…é‡")
                        recommendations.append("5. æ§åˆ¶é¥®é£Ÿï¼Œå¢åŠ è¿åŠ¨ï¼Œç»´æŒå¥åº·ä½“é‡")
                    elif isinstance(mean_bmi, (int, float)):
                        bmi_status = "è‚¥èƒ–"
                        health_issues.append("è‚¥èƒ–")
                        recommendations.extend([
                            "5. åˆ¶å®šå¥åº·å‡é‡è®¡åˆ’ï¼Œæ§åˆ¶çƒ­é‡æ‘„å…¥",
                            "6. å¢åŠ ä½“è‚²æ´»åŠ¨ï¼Œæ¯å‘¨è‡³å°‘150åˆ†é’Ÿä¸­ç­‰å¼ºåº¦è¿åŠ¨",
                            "7. å’¨è¯¢è¥å…»å¸ˆï¼Œåˆ¶å®šä¸ªæ€§åŒ–é¥®é£Ÿè®¡åˆ’"
                        ])
                    
                    if bmi_status:
                        group_type += f" - {bmi_status}"
                
                # è¯„ä¼°è¡€å‹
                if has_bp:
                    bp_status = ""
                    if (isinstance(mean_systolic, (int, float)) and isinstance(mean_diastolic, (int, float)) and 
                        (mean_systolic >= 140 or mean_diastolic >= 90)):
                        bp_status = "é«˜è¡€å‹é£é™©"
                        health_issues.append("é«˜è¡€å‹é£é™©")
                        recommendations.extend([
                            "8. é™åˆ¶é’ ç›æ‘„å…¥ï¼Œæ¯å¤©æ‘„å…¥é‡æ§åˆ¶åœ¨5gä»¥ä¸‹",
                            "9. å¢åŠ é’¾çš„æ‘„å…¥ï¼Œå¤šåƒæ–°é²œè”¬æœ",
                            "10. æˆ’çƒŸé™é…’ï¼Œé¿å…è¿‡åº¦åŠ³ç´¯",
                            "11. å­¦ä¹ è¡€å‹è‡ªæˆ‘ç›‘æµ‹ï¼Œä¿æŒè®°å½•"
                        ])
                    elif (isinstance(mean_systolic, (int, float)) and isinstance(mean_diastolic, (int, float)) and 
                          (mean_systolic <= 90 or mean_diastolic <= 60)):
                        bp_status = "ä½è¡€å‹é£é™©"
                        health_issues.append("ä½è¡€å‹é£é™©")
                        recommendations.extend([
                            "8. é¿å…çªç„¶ç«™ç«‹ï¼Œé¢„é˜²ä½“ä½æ€§ä½è¡€å‹",
                            "9. ä¿æŒæ°´åˆ†æ‘„å…¥ï¼Œé˜²æ­¢è„±æ°´",
                            "10. é€‚å½“å¢åŠ ç›åˆ†æ‘„å…¥ï¼ˆåœ¨åŒ»ç”ŸæŒ‡å¯¼ä¸‹ï¼‰"
                        ])
                    
                    if bp_status:
                        group_type += f" - {bp_status}"
                
                # è¾“å‡ºåˆ°æ–‡ä»¶
                f.write(f"ç¾¤ä½“ {cluster+1}: {group_type} (æ ·æœ¬æ•°: {len(cluster_data)})\n\n")
                f.write("ç‰¹å¾æ¦‚å†µ:\n")
                f.write(f"- å¹´é¾„å‡å€¼: {mean_age:.1f} å²\n")
                f.write(f"- æ€§åˆ«åˆ†å¸ƒ: {gender_str}\n")
                
                if has_bmi:
                    f.write(f"- BMIå‡å€¼: {mean_bmi:.1f}\n")
                
                if has_bp:
                    f.write(f"- æ”¶ç¼©å‹å‡å€¼: {mean_systolic:.1f} mmHg\n")
                    f.write(f"- èˆ’å¼ å‹å‡å€¼: {mean_diastolic:.1f} mmHg\n")
                
                f.write("\næ½œåœ¨å¥åº·é—®é¢˜:\n")
                if health_issues:
                    for issue in health_issues:
                        f.write(f"- {issue}\n")
                else:
                    f.write("- æ— æ˜æ˜¾å¥åº·é£é™©\n")
                
                f.write("\nå¥åº·ç®¡ç†å»ºè®®:\n")
                for rec in recommendations:
                    f.write(f"{rec}\n")
                f.write("\n" + "=" * 40 + "\n\n")
                
            except Exception as e:
                f.write(f"ç¾¤ä½“ {cluster+1}: æ•°æ®ä¸è¶³ä»¥è¿›è¡Œè¯¦ç»†åˆ†æ\n")
                f.write(f"é”™è¯¯: {str(e)}\n\n")
    
    print(f"å¥åº·ç¾¤ä½“åˆ†æå’Œå»ºè®®å·²ä¿å­˜åˆ°: {os.path.join(results_dir, f'{algorithm_name}_health_recommendations.txt')}")

def main():
    # åŠ è½½æ•°æ®
    df = load_data(sample_size=5000)
    
    # é¢„å¤„ç†æ•°æ®
    data, scaled_data, features = preprocess_data(df)
    
    # é™ç»´ç”¨äºå¯è§†åŒ–
    reduced_data = reduce_dimensions(scaled_data)
    
    # 1. Kå‡å€¼èšç±»
    kmeans_clusters = kmeans_clustering(scaled_data)
    visualize_clusters(data, reduced_data, kmeans_clusters, "kmeans", features)
    
    # 2. å±‚æ¬¡èšç±»
    hierarchical_clusters = hierarchical_clustering(scaled_data)
    visualize_clusters(data, reduced_data, hierarchical_clusters, "hierarchical", features)
    
    # 3. DBSCANèšç±» - è‡ªåŠ¨è°ƒæ•´å‚æ•°
    # DBSCANå‚æ•°è°ƒæ•´ä¸ºæ›´é€‚åˆçš„å€¼
    eps = 0.8  # å¢åŠ é‚»åŸŸåŠå¾„
    min_samples = 15  # å‡å°‘æœ€å°æ ·æœ¬æ•°
    dbscan_clusters = dbscan_clustering(scaled_data, eps, min_samples)
    
    # å¦‚æœDBSCANèšç±»ç»“æœä¸å¥½ï¼ˆå¤§éƒ¨åˆ†ä¸ºå™ªå£°ç‚¹ï¼‰ï¼Œå°è¯•å†æ¬¡è°ƒæ•´å‚æ•°
    noise_ratio = np.sum(dbscan_clusters == -1) / len(dbscan_clusters)
    if noise_ratio > 0.5:  # å¦‚æœè¶…è¿‡50%æ˜¯å™ªå£°ç‚¹
        print("DBSCANç”Ÿæˆè¿‡å¤šå™ªå£°ç‚¹ï¼Œå°è¯•è°ƒæ•´å‚æ•°...")
        eps = 1.0  # è¿›ä¸€æ­¥å¢åŠ é‚»åŸŸåŠå¾„
        min_samples = 10  # è¿›ä¸€æ­¥å‡å°‘æœ€å°æ ·æœ¬æ•°
        dbscan_clusters = dbscan_clustering(scaled_data, eps, min_samples)
    
    visualize_clusters(data, reduced_data, dbscan_clusters, "dbscan", features)
    
    # 4. è°±èšç±»
    try:
        print(f"\nğŸ” æ‰§è¡Œè°±èšç±»åˆ†æ " + "="*48)
        print(f"  Â· ç®—æ³•ç‰¹ç‚¹: åŸºäºå›¾è®ºå’Œæµå½¢å­¦ä¹ çš„èšç±»æ–¹æ³•ï¼Œèƒ½å¤Ÿå‘ç°éå‡¸å½¢çŠ¶çš„ç°‡")
        print(f"  Â· ä¼˜åŠ¿: èƒ½å¤„ç†å¤æ‚å½¢çŠ¶çš„ç°‡ï¼Œå¯¹å™ªå£°ç›¸å¯¹é²æ£’")
        print(f"  Â· åŠ£åŠ¿: è®¡ç®—å¤æ‚åº¦é«˜ï¼Œå¤§æ•°æ®é›†ä¸Šæ€§èƒ½è¾ƒå·®")
        print(f"  Â· å®ç°æ–¹æ³•: ä½¿ç”¨æœ€è¿‘é‚»å›¾æ„å»ºç›¸ä¼¼åº¦çŸ©é˜µï¼Œé™ä½è®¡ç®—å¤æ‚åº¦")
        
        print(f"\n  å¼€å§‹è®­ç»ƒè°±èšç±»æ¨¡å‹...")
        spectral_clusters = spectral_clustering(scaled_data)
        
        # ç»Ÿè®¡æ¯ä¸ªç°‡çš„æ ·æœ¬æ•°
        unique, counts = np.unique(spectral_clusters, return_counts=True)
        cluster_counts = dict(zip(unique, counts))
        
        # è¾“å‡ºç»“æœç»Ÿè®¡
        print(f"\n  ğŸ“Š èšç±»ç»“æœ:")
        print(f"    - æ€»æ ·æœ¬æ•°: {len(spectral_clusters)}")
        print(f"    - èšç±»æ•°é‡: {len(unique)}")
        
        for i in range(len(unique)):
            print(f"    - ç¾¤ä½“ {i+1}: {cluster_counts.get(i, 0)}ä¸ªæ ·æœ¬ " + 
                  f"({cluster_counts.get(i, 0)/len(spectral_clusters)*100:.1f}%)")
        
        # å¯è§†åŒ–ç»“æœ
        visualize_clusters(data, reduced_data, spectral_clusters, "spectral", features)
        print(f"\n  âœ… è°±èšç±»æ‰§è¡ŒæˆåŠŸå¹¶ç”Ÿæˆå¯è§†åŒ–ç»“æœ")
    except Exception as e:
        print(f"\n  âŒ è°±èšç±»æ‰§è¡Œå¤±è´¥: {str(e)}")
        print(f"     å¯èƒ½åŸå› : å†…å­˜ä¸è¶³æˆ–æ•°æ®é›†è¿‡å¤§ï¼Œè°±èšç±»çš„è®¡ç®—å¤æ‚åº¦è¾ƒé«˜")
    
    print("\næ‰€æœ‰èšç±»ç®—æ³•æ‰§è¡Œå®Œæ¯•ï¼Œç»“æœå·²ä¿å­˜åˆ°resultsç›®å½•")

if __name__ == "__main__":
    main() 